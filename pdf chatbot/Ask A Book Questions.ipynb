{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b715375",
   "metadata": {},
   "source": [
    "# Top-K Similarity Search - Ask A Book A Question\n",
    "\n",
    "In this tutorial we will see a simple example of basic retrieval via Top-K Similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d615a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain --upgrade\n",
    "# Version: 0.0.164\n",
    "\n",
    "# !pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d3e92ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anand\\Desktop\\ananda github\\Langchain-tutorial\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PDF Loaders. If unstructured gives you a hard time, try PyPDFLoader\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5166d759",
   "metadata": {},
   "source": [
    "### Load your data\n",
    "\n",
    "Next let's load up some data. I've put a few 'loaders' on there which will load data from different locations. Feel free to use the one that suits you. The default one queries one of Paul Graham's essays for a simple example. This process will only stage the loader, not actually load it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d38044",
   "metadata": {},
   "source": [
    "Then let's go ahead and actually load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59b93415",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"Net Centric Computing chapter 1 solutions.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcdac23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07a744a",
   "metadata": {},
   "source": [
    "Then let's actually check out what's been loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4fd7c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 33 document(s) in your data\n",
      "There are 1080 characters in your sample document\n",
      "Here is a sample: Net Centric Computing (dot net) \n",
      "Old questions Solutions Chapter 1 \n",
      "BSC CSIT \n",
      "1. Create class to showcase constructor, properties, indexers and encapsulation behavior of \n",
      "object-oriented language. [mo\n"
     ]
    }
   ],
   "source": [
    "# Note: If you're using PyPDFLoader then it will split by page for you already\n",
    "print (f'You have {len(data)} document(s) in your data')\n",
    "print (f'There are {len(data[0].page_content)} characters in your sample document')\n",
    "print (f'Here is a sample: {data[0].page_content[:200]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af9b604",
   "metadata": {},
   "source": [
    "### Chunk your data up into smaller documents\n",
    "\n",
    "While we could pass the entire essay to a model w/ long context, we want to be picky about which information we share with our model. The better signal to noise ratio we have the more likely we are to get the right answer.\n",
    "\n",
    "The first thing we'll do is chunk up our document into smaller pieces. The goal will be to take only a few of those smaller pieces and pass them to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb3c6f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll split our data into chunks around 500 characters each with a 50 character overlap. These are relatively small.\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "879873a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 99 documents\n"
     ]
    }
   ],
   "source": [
    "# Let's see how many small chunks we have\n",
    "print (f'Now you have {len(texts)} documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838b2843",
   "metadata": {},
   "source": [
    "### Create embeddings of your documents to get ready for semantic search\n",
    "\n",
    "Next up we need to prepare for similarity searches. The way we do this is through embedding our documents (getting a vector per document).\n",
    "\n",
    "This will help us compare documents later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "373e695a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anand\\AppData\\Local\\Temp\\ipykernel_71932\\30168171.py:8: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def download_embeddings():\n",
    "    \"\"\"\n",
    "    Download and return the HuggingFace embeddings model.\n",
    "    \"\"\"\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "embedding = download_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884e7857",
   "metadata": {},
   "source": [
    "Check to see if there is an environment variable with you API keys, if not, use what you put below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d66c06",
   "metadata": {},
   "source": [
    "### Option #1: Chroma (for local)\n",
    "\n",
    "I like Chroma becauase it's local and easy to set up without an account.\n",
    "\n",
    "First we'll pass our texts to Chroma via `.from_documents`, this will 1) embed the documents and get a vector, then 2) add them to the vectorstore for retrieval later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e0d1c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it into Chroma\n",
    "from langchain.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(texts, embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4750ab",
   "metadata": {},
   "source": [
    "Let's test it out. I want to see which documents are most closely related to a query.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34929595",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What peter thiel thinks about startups?\"\n",
    "docs = vectorstore.similarity_search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec60de1",
   "metadata": {},
   "source": [
    "Then we can check them out. In theory, the texts which are deemed most similar should hold the answer to our question.\n",
    "But keep in mind that our query just happens to be a question, it could be a random statement or sentence and it would still work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e0f5b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "national security and global finance. He has provided early funding for LinkedIn, Yelp, and dozens of\n",
      "successful technology startups, many run by former colleagues who have been dubbed the “PayPal\n",
      "Mafia.” He is a partner at Founders Fund, a Silicon Valley venture capital firm that has funded\n",
      "companies like SpaceX and Airbnb. He started the Thiel Fellowship, which ignited a national debate\n",
      "by encouraging young people to put learning before schooling, and he leads the Thiel Foundation,\n",
      "\n",
      "About the Authors\n",
      "Peter Thiel\n",
      " is an entrepreneur and investor. He started PayPal in 1998, led it as CEO, and took it public in\n",
      "2002, defining a new era of fast and secure online commerce. In 2004 he made the first outside\n",
      "investment in Facebook, where he serves as a director. The same year he launched Palantir\n",
      "Technologies, a software company that harnesses computers to empower human analysts in fields like\n",
      "\n",
      "but he could never create an entire industry. Startups operate on the principle that you need to work\n",
      "with other people to get stuff done, but you also need to stay small enough so that you actually can.\n",
      "Positively defined, a startup is the largest group of people you can convince of a plan to build a\n",
      "different future. A new company’s most important strength is new thinking: even more important than\n",
      "nimbleness, small size affords space to\n",
      "\n",
      "FOUNDATIONS\n",
      "E\n",
      "VERY GREAT COMPANY\n",
      " is unique, but there are a few things that every business must get right at the beginning.\n",
      "I stress this so often that friends have teasingly nicknamed it “Thiel’s law”: \n",
      "a startup messed up at\n",
      "its foundation cannot be fixed\n",
      ".\n",
      "Beginnings are special. They are qualitatively different from all that comes afterward. This was\n",
      "true 13.8 billion years ago, at the founding of our cosmos: in the earliest microseconds of its\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here's an example of the first document that was returned\n",
    "for doc in docs:\n",
    "    print (f\"{doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73d8504",
   "metadata": {},
   "source": [
    "### Option #2: Pinecone (for cloud)\n",
    "If you want to use pinecone, run the code below, if not then skip over to Chroma below it. You must go to [Pinecone.io](https://www.pinecone.io/) and set up an account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e093ef3",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# PINECONE_API_KEY = os.getenv('PINECONE_API_KEY', 'YourAPIKey')\n",
    "# PINECONE_API_ENV = os.getenv('PINECONE_API_ENV', 'us-east1-gcp') # You may need to switch with your env\n",
    "\n",
    "# # initialize pinecone\n",
    "# pinecone.init(\n",
    "#     api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
    "#     environment=PINECONE_API_ENV  # next to api key in console\n",
    "# )\n",
    "# index_name = \"langchaintest\" # put in the name of your pinecone index here\n",
    "\n",
    "# docsearch = Pinecone.from_texts([t.page_content for t in texts], embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c35dcd9",
   "metadata": {},
   "source": [
    "### Query those docs to get your answer back\n",
    "\n",
    "Great, those are just the docs which should hold our answer. Now we can pass those to a LangChain chain to query the LLM.\n",
    "\n",
    "We could do this manually, but a chain is a convenient helper for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f051337b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",\n",
    "    temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b9b1c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anand\\AppData\\Local\\Temp\\ipykernel_71932\\4041488970.py:1: LangChainDeprecationWarning: This class is deprecated. See the following migration guides for replacements based on `chain_type`:\n",
      "stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\n",
      "map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\n",
      "refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\n",
      "map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\n",
      "\n",
      "See also guides on retrieval and question-answering here: https://python.langchain.com/docs/how_to/#qa-with-rag\n",
      "  chain = load_qa_chain(llm, chain_type=\"stuff\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f67ea7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Net Centric Computing chapter 1 solutions.pdf', 'author': \"I'M Adarsha\", 'creationdate': '2025-10-21T18:01:58+05:45', 'moddate': '2025-10-21T18:01:58+05:45', 'total_pages': 33, 'page': 24, 'creator': 'Microsoft® Word LTSC', 'page_label': '25', 'producer': 'Microsoft® Word LTSC'}, page_content='It uses async and await keywords, which make the code non-blocking, faster, and more \\nresponsive — especially useful in desktop apps, web servers, and network programming. \\nNeed for Asynchronous Programming: \\n1. Non-blocking execution: \\nThe main thread (UI or main program) continues working while waiting for time-\\nconsuming tasks to complete. \\n2. Improved performance: \\nIt helps make programs faster by using system resources efficiently. \\n3. Better user experience:'),\n",
       " Document(metadata={'page_label': '25', 'source': 'Net Centric Computing chapter 1 solutions.pdf', 'creator': 'Microsoft® Word LTSC', 'moddate': '2025-10-21T18:01:58+05:45', 'producer': 'Microsoft® Word LTSC', 'page': 24, 'creationdate': '2025-10-21T18:01:58+05:45', 'total_pages': 33, 'author': \"I'M Adarsha\"}, page_content='3. Better user experience: \\nIn UI applications, asynchronous code prevents the screen from freezing while \\nbackground work (like downloading files) is in progress. \\n4. Efficient resource use: \\nThe CPU and memory can handle other tasks while waiting for I/O operations. \\n \\nExample: \\nusing System; \\nusing System.Threading.Tasks; \\n \\nclass Program \\n{ \\n    static async Task Main() \\n    { \\n        Console.WriteLine(\"Downloading started...\"); \\n        await DownloadFileAsync();  // Runs asynchronously'),\n",
       " Document(metadata={'author': \"I'M Adarsha\", 'page_label': '25', 'producer': 'Microsoft® Word LTSC', 'creationdate': '2025-10-21T18:01:58+05:45', 'total_pages': 33, 'moddate': '2025-10-21T18:01:58+05:45', 'source': 'Net Centric Computing chapter 1 solutions.pdf', 'page': 24, 'creator': 'Microsoft® Word LTSC'}, page_content='question -5 marks] \\nLambda expression: see above solution \\nNeed of asynchronous programming: \\nAsynchronous programming in C# allows a program to perform multiple tasks at the same time \\nwithout blocking the main thread. It is mainly used when one operation (like file access, API \\ncalls, or downloading data) takes a long time to complete. Instead of waiting for that operation \\nto finish, the program continues running other tasks.'),\n",
       " Document(metadata={'moddate': '2025-10-21T18:01:58+05:45', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2025-10-21T18:01:58+05:45', 'source': 'Net Centric Computing chapter 1 solutions.pdf', 'page': 25, 'total_pages': 33, 'page_label': '26', 'author': \"I'M Adarsha\", 'producer': 'Microsoft® Word LTSC'}, page_content='} \\n \\n    static async Task DownloadFileAsync() \\n    { \\n        await Task.Delay(3000); // Simulates time-consuming task \\n        Console.WriteLine(\"File downloaded.\"); \\n    } \\n} \\n \\nHere, the Main() method calls the DownloadFileAsync() method using the await keyword. \\nThe program does not freeze during the 3-second delay; it continues running smoothly. \\nThis demonstrates how asynchronous programming helps perform time-consuming tasks \\nwithout blocking the main program.')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"give me the exampe of asynchronous programming?\"\n",
    "docs = vectorstore.similarity_search(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516e0a80",
   "metadata": {},
   "source": [
    "Awesome! We just went and queried an external data source!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c3d1062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here\\'s an example of asynchronous programming from the provided context:\\n\\n```csharp\\nusing System;\\nusing System.Threading.Tasks;\\n\\nclass Program\\n{\\n    static async Task Main()\\n    {\\n        Console.WriteLine(\"Downloading started...\");\\n        await DownloadFileAsync();  // Runs asynchronously\\n    }\\n\\n    static async Task DownloadFileAsync()\\n    {\\n        await Task.Delay(3000); // Simulates time-consuming task\\n        Console.WriteLine(\"File downloaded.\");\\n    }\\n}\\n```\\n\\n**Explanation:**\\n\\nIn this example:\\n\\n*   The `Main()` method calls the `DownloadFileAsync()` method using the `await` keyword.\\n*   `DownloadFileAsync()` simulates a time-consuming task (like downloading a file) by pausing for 3 seconds using `Task.Delay(3000)`.\\n*   During this 3-second delay, the program does not freeze. The `await` keyword ensures that the main thread is not blocked, allowing it to remain responsive (even though in this simple console app, there isn\\'t much else for the main thread to do, it illustrates the non-blocking principle).\\n*   Once the `DownloadFileAsync()` task completes, the message \"File downloaded.\" is printed.\\n\\nThis demonstrates how `async` and `await` allow for performing time-consuming tasks without blocking the main program\\'s execution.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db3c060",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UV Python 3.11 (langchain-tutorial)",
   "language": "python",
   "name": "langchain-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
