{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7xhZfK_6T2G",
        "outputId": "beef290a-7e0a-446e-8cca-c0ba9eafceb1"
      },
      "source": [
        "!pip install langchain rank_bm25 pypdf unstructured chromadb\n",
        "!pip install unstructured['pdf'] unstructured\n",
        "!apt-get install poppler-utils\n",
        "!apt-get install -y tesseract-ocr\n",
        "!apt-get install -y libtesseract-dev\n",
        "!pip install pytesseract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "monR9xmw6p2E"
      },
      "source": [
        "### Load the required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hJ7NC1yB6jDX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\anand\\Rag\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from langchain.document_loaders import UnstructuredPDFLoader,PyPDFLoader\n",
        "from langchain.text_splitter import  RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "from langchain.llms import HuggingFaceHub\n",
        "\n",
        "\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWcedXWq7unE"
      },
      "outputs": [],
      "source": [
        "### Load the PDF file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "B1II05VC6vjr"
      },
      "outputs": [],
      "source": [
        "\n",
        "data_file = PyPDFLoader(\"CG-1.pdf\")\n",
        "docs = data_file.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqUCmzkF7xOe",
        "outputId": "6462eb53-ba6e-44f6-e807-1f5a3b557716"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://collegenote.pythonanywhere.com                                     Prepared By: Jayanta Poudel \n",
            " \n",
            "1 Computer Graphics (Reference Note)                                                                      BSc.CSIT                                                                                                   \n",
            "Unit 1 \n",
            "Introduction of Computer Graphics \n",
            "Computer graphics is a field related to the generation of graphics using computer. It includes \n",
            "the creation, storage and manipulation of images of object. These objects come from diverse \n",
            "field such as medicine, physical, mathematical, engineering, architecture, entertainment, \n",
            "advertisement. \n",
            "- It is related to the generation and the representation of graphics by a computer using \n",
            "specialized graphic hardware and software. The graphics can be photographs, drawings, \n",
            "movies, or simulation etc. \n",
            "- Computer graphics today is largely interactive; that is the user controls the contents \n",
            "structure and appearance of images of the objects by using input devices such as \n",
            "keyboard, mouse, or touch sensitive panel on the screen. \n",
            " \n",
            "Main task \n",
            "Imaging: Formation of an image. \n",
            "        -  representation of 2D images. \n",
            "Modelling: Representing 3D images. \n",
            "Rendering: Constructing 2D images from 3D models. \n",
            "Animation: Stimulating changes over time. \n",
            "           -  describing how objects change in time. \n",
            " \n",
            "Basic Concepts \n",
            "Raster: A rectangular array of points and dots. \n",
            "Pixel (picture element): One dot or picture element of the raster. \n",
            "Scan line: A row of pixel. \n",
            "Bit map: Ones and Zeros representation of rectangular array of point on the screen. \n",
            " Black & white → bitmap \n",
            " Color→ pixmap \n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            "        \n",
            " \n",
            "Note: we don’t have any pixel like 1.2, 5.8. \n",
            "   Raster device co-ordinate can have only integer values. \n",
            "Scan line \n",
            "n-1 \n",
            "0 \n",
            " \n",
            "m×n raster \n",
            " \n",
            "m-1 \n",
            " \n",
            "Pixel\n"
          ]
        }
      ],
      "source": [
        "print(docs[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "31411\n"
          ]
        }
      ],
      "source": [
        "total_chars = sum(len(docs.page_content) for docs in docs)\n",
        "print(total_chars)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZk8Ob1U8DZK"
      },
      "source": [
        "### Split Documents and Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qYZ1j1Ns73mU"
      },
      "outputs": [],
      "source": [
        "# create chunks\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=800,\n",
        "                                          chunk_overlap=100)\n",
        "chunks = splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "3o58WVXM8SvN",
        "outputId": "f5c0cd8d-4c23-4c6b-c240-15b9cc340edb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "54"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(chunks[1].page_content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ch3SR3Wu8Tnw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\anand\\AppData\\Local\\Temp\\ipykernel_482176\\614034910.py:8: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(\n"
          ]
        }
      ],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "def download_embeddings():\n",
        "    \"\"\"\n",
        "    Download and return the HuggingFace embeddings model.\n",
        "    \"\"\"\n",
        "    model_name = \"BAAI/bge-base-en-v1.5\"\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "        model_name=model_name\n",
        "    )\n",
        "    return embeddings\n",
        "\n",
        "embedding = download_embeddings()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfB7uezt9Vud"
      },
      "source": [
        "### VectorStore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GeIka0wR87kF"
      },
      "outputs": [],
      "source": [
        "# Vector store with the selected embedding model\n",
        "from langchain.vectorstores import Chroma\n",
        "vectorstore = Chroma.from_documents(chunks, embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YaaxtuWw9uJM"
      },
      "outputs": [],
      "source": [
        "vectorstore_retreiver = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000002C433E673D0>, search_kwargs={'k': 3})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectorstore_retreiver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nMucCA4z9VAn"
      },
      "outputs": [],
      "source": [
        "keyword_retriever = BM25Retriever.from_documents(chunks)\n",
        "keyword_retriever.k =  3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "BM25 Retriever is a sparse retrieval method that uses the BM25 (Best Matching 25) algorithm, which is a classic and highly effective information retrieval technique.\n",
        "\n",
        "What is BM25?\n",
        "BM25 is a bag-of-words retrieval function that ranks documents based on query terms appearing in each document. It's considered one of the most effective traditional retrieval algorithms.\n",
        "\n",
        "How BM25 Works\n",
        "Unlike vector stores that use dense embeddings, BM25:\n",
        "\n",
        "Sparse Representation: Uses term frequency and document statistics\n",
        "\n",
        "Keyword-based: Relies on exact word matching (but with smart weighting)\n",
        "\n",
        "Statistical Approach: Considers:\n",
        "\n",
        "Term frequency (TF) in documents\n",
        "\n",
        "Inverse document frequency (IDF)\n",
        "\n",
        "Document length normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3UCqmV5-D-r"
      },
      "source": [
        "### Ensemble Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nVhLxoWh95dv"
      },
      "outputs": [],
      "source": [
        "ensemble_retriever = EnsembleRetriever(retrievers=[vectorstore_retreiver,\n",
        "                                                   keyword_retriever],\n",
        "                                       weights=[0.5, 0.5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "AG1h9tvE-KqH"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",\n",
        "    temperature=0.7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk-bQqY8-kUH"
      },
      "source": [
        "### Prompt Template:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "0C8Vn8PR-QJo"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"\n",
        "<|system|>>\n",
        "You are a helpful AI Assistant that follows instructions extremely well.\n",
        "Use the following context to answer user question.\n",
        "\n",
        "Think step by step before answering the question. You will get a $100 tip if you provide correct answer.\n",
        "\n",
        "CONTEXT: {context}\n",
        "</s>\n",
        "<|user|>\n",
        "{query}\n",
        "</s>\n",
        "<|assistant|>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "c8eDFcNd_Kru"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "output_parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5JtvKJk2_Ok2"
      },
      "outputs": [],
      "source": [
        "chain = (\n",
        "    {\"context\": ensemble_retriever, \"query\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | output_parser\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubQmwtOn_S6U",
        "outputId": "b04b52d3-c39a-42bd-bb87-b4ec246aaea0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To determine if the given rate will produce an un-flicking effect, we need to calculate the total time required to access all pixels on the screen and compare it to the typical refresh rate needed to avoid flickering.\n",
            "\n",
            "1.  **Calculate the total number of pixels on the screen:**\n",
            "    Screen size = 640 × 480\n",
            "    Total no. of pixels = 640 * 480 = 307,200 pixels\n",
            "\n",
            "2.  **Calculate the total time required to access all pixels for one frame:**\n",
            "    Average access time of one pixel = 300 ns\n",
            "    Total time = Total no. of pixels * Average access time per pixel\n",
            "    Total time = 307,200 * 300 ns = 92,160,000 ns\n",
            "\n",
            "3.  **Convert the total time to seconds:**\n",
            "    1 second = 10^9 ns\n",
            "    Total time = 92,160,000 / 10^9 seconds = 0.09216 seconds\n",
            "\n",
            "4.  **Compare with the required refresh rate for an un-flicking effect:**\n",
            "    To produce an un-flicking effect, a screen typically needs a refresh rate of at least 60 frames per second (Hz).\n",
            "    This means the time taken to refresh one frame should be:\n",
            "    Time per frame (for un-flicking) = 1 second / 60 frames ≈ 0.01667 seconds\n",
            "\n",
            "Since the calculated total time to access all pixels (0.09216 seconds) is significantly longer than the required time per frame for an un-flicking effect (0.01667 seconds), this rate **will not** produce an un-flicking effect. The screen would refresh at approximately 1 / 0.09216 ≈ 10.85 frames per second, which is too slow to prevent flickering.\n"
          ]
        }
      ],
      "source": [
        "print(chain.invoke(\" If a pixel is accessed from the frame buffer with an average access time of 300ns then will this rate produce an un-flicking effect for the screen size of 640 × 480. \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75_mkqLxOIzg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iow-D11UOI3G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSXb9AmYOI7O"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmgGoENINoAr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZN6wb_AiNoDq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YVQ1xW3_XNt",
        "outputId": "4665e541-0620-470a-cbb9-1502c1570721"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Human: \n",
            "<|system|>>\n",
            "You are a helpful AI Assistant that follows instructions extremely well.\n",
            "Use the following context to answer user question. \n",
            "\n",
            "Think step by step before answering the question. You will get a $100 tip if you provide correct answer. \n",
            "\n",
            "CONTEXT: [Document(page_content='While significantly better than Vicuna and marginally better than ChatGPT, Orca’s average performance of 49.7%, lags GPT-4 by 26%. Note that GPT-4 has reported a data contami- nation issue with Big-Bench and that we are not aware of such issues with either LLaMA’s training data (the base model used by both Vicuna and Orca) or the Flan-V2 collection or Vicuna’s training data (ShareGPT).\\n\\nGiven the close performance on average on BigBench-Hard, we take a deeper look at differences in performance between Orca and ChatGPT:\\n\\nEntailment and Semantic Understanding:\\n\\nOrca performs better at entailment (formal fallacies) and semantic understanding (Dis- ambiguation QA and Snarks).', metadata={'source': './data/Orca_paper.pdf'}), Document(page_content='How does Steven feel about selling? Extract from the transcript the most relevant segments for the answer, and then answer the question.', metadata={'source': './data/Orca_paper.pdf'}), Document(page_content='Figure 26: Case Study 7: Compound Interest Problems\\n\\n11.8 Question from Vicuna-Eval\\n\\n### System: You are an AI assistant that follows instruction extremely well. Help as much as you can. ### Human: How many times does the average human blink in a lifetime? Try to explain your answer. Your explanation should take the reader through your reasoning step-by-step.\\n\\n### Orca: To determine the average number of times a human blinks in a lifetime, we need to consider the following factors:\\n\\n1. Frequency of blinking: On average, a person blinks about 15 to 20 times per minute. This number can vary depending on factors such as age, environmental conditions, and individual habits.\\n\\n2. Lifespan: The average human lifespan is approximately 72 years.', metadata={'source': './data/Orca_paper.pdf'}), Document(page_content='For the first factor, x: x = 0 So, root1 = 0.\\n\\nFor the second factor, (x + 3.0): x + 3.0 = 0 x = -3.0 So, root2 = -3.0.\\n\\nStep 4: Present the roots The roots of the equation x2 + 3.0x = 0 are 0 and -3.0.\\n\\nROOTS = [0, -3.0]\\n\\nFigure 32: Case Study 12: Quadratic equation solving from the blog Exploring ChatGPT vs open-source models on slightly harder tasks\\n\\n11.13 Meeting Transcript Processing\\n\\n### System: You are a teacher. Given a task, you explain in simple steps what the task is asking, any guidelines it provides and how to use those guidelines to find the answer. ### Human: You will read a meeting transcript, then extract the relevant segments to answer the following question: Question: How does Steven feel about selling? Here is a meeting transcript: —-\\n\\n46', metadata={'source': './data/Orca_paper.pdf'}), Document(page_content='ChatGPT has better geometric reasoning capabilities than Orca as measured by geometric shape task (predicting shape from a full SVG path element). ChatGPT outperforms Orca by 23% on this task, which highlights the lack of geometric reasoning capabilities of Orca compared to ChatGPT.\\n\\nTable Understanding:\\n\\nChatGPT has better table understanding and reasoning capabilities than Orca. • In the penguins in a table task (answering questions based on understanding facts in a table), Orca lags behind ChatGPT by 7.4%, thereby highlighting Orca’s poor table understanding and reasoning capabilities compared to ChatGPT.\\n\\n22', metadata={'source': './data/Orca_paper.pdf'})]\n",
            "</s>\n",
            "<|user|>\n",
            "How does Orca compares to ChatGPT?\n",
            "</s>\n",
            "<|assistant|>\n",
            "Based on the context provided, Orca's performance on certain tasks is compared to ChatGPT. Here's a summary of their comparisons:\n",
            "\n",
            "1. Geometric reasoning capabilities: ChatGPT outperforms Orca by 23% on the geometric shape task, indicating that ChatGPT has better geometric reasoning capabilities than Orca.\n",
            "\n",
            "2. Table understanding and reasoning capabilities: In the penguins in a table task, Orca lags behind ChatG\n"
          ]
        }
      ],
      "source": [
        "print(chain.invoke(\"How does Orca compares to ChatGPT?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fPGNd-aV__1x"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain_groq'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_groq\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatGroq\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Initialize the model\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_groq'"
          ]
        }
      ],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Initialize the model\n",
        "llm = ChatGroq(model=\"llama3-8b-8192\", temperature=0.7)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\anand\\Rag\\.venv\\Scripts\\python.exe: No module named pip\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
